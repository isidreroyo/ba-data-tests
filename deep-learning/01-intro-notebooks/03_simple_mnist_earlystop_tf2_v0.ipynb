{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKfnXKeITozd"
      },
      "source": [
        "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDbuKRt0Toze"
      },
      "source": [
        "## Primer ejemplo de Red Neuronal para el Master de Data Science de Nuclio School con EarlyStop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-T54d2dToze"
      },
      "source": [
        "Antes de entrar en el tema seria bueno que vieras el video de introducción al tema de EarlyStopping\n",
        "\n",
        "https://drive.google.com/file/d/1cwGEv4a2BgDLRXHGqglOZD_MWW3y9Fuv/view?usp=drive_link\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ii4N2FTozf"
      },
      "source": [
        "Recordemos los pasos a seguiren la creación de código para entrenar una red neuronal:\n",
        "<ol>\n",
        "    <li>Importar librerias: Keras, PyPlot y Numpy (añado time y datetime para controlar los tiempos de entrenamiento)</li>\n",
        "    <li>Definamos una red inicial (apilando bloques)</li>\n",
        "    <li>Definimos (o creamos) nuestro optimizador, añadiendo nuestra función error</li>\n",
        "    <li>Preparamos los datos (en este caso los cargamos)</li>\n",
        "    <li>Empezamos con el entrenamiento</li>\n",
        "    <li>Miramos los resultados y pasamos a la crítica mordaz</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FK1A67QTozf"
      },
      "source": [
        "## 1. Librerias\n",
        "\n",
        "Para empezar carguemos esas librerias que nos hacen falta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze0-DntvTozf"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras as ks\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Cargamos la libreria de EarlyStop\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nYxQCgTTozg"
      },
      "source": [
        "## 2. Arquitectura de red del modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZFL1PlNTozg"
      },
      "outputs": [],
      "source": [
        "model = ks.Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape=(28,28)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwC6J9R-Tozg"
      },
      "source": [
        "Para revisar un modelo, nos basta con llamar al método **.summary()** del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NTffCVETozh"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nfbeRQNTozh"
      },
      "source": [
        "## 3. Optimizador, función error\n",
        "\n",
        "Definimos los parametros del modelo para su entrenamiento:\n",
        "* **Loss** - Función de error (función de coste) - Optamos por la Sparse Categorical Crosstentropy porque estamos clasificando imagenes\n",
        "* **Optimizer** - que optimizador de la función de coste usaremos, en este ejemplo Adam\n",
        "* **Metrics** - que metrica usaremos para evaluar el modelo... en este caso se usa la Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j7ovChITozh"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'], )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNIHVajETozh"
      },
      "source": [
        "## 4. Preparamos los datos\n",
        "\n",
        "Cargamos los datos de MNIST de los datasets directamente de las librerias de Keras. Estos ya estan dispuestos en train and test\n",
        "\n",
        "**Detalle importante:**\n",
        "> La red neuronal requiere que los inputs sean números reales, y lo haremos forzando la division de los valores de dentro de las matrices 28x28 (que tienen valoress del 0 al 255) por 255.0 (un real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFJqNITwTozh"
      },
      "outputs": [],
      "source": [
        "mnist = ks.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxeL2yMzTozh"
      },
      "source": [
        "Pintemos una muestra de las imagenes del dataset MNIST, a ver si se parece en algo a lo que esperamos.\n",
        "Primero, vemos que tipos de datos tengo, después mapeamos esas matrices en una escala de grises utilizando el método **.get_cmap()** de PlotLy con los nueve primeros números del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCPQTi71Tozi"
      },
      "outputs": [],
      "source": [
        "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
        "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))\n",
        "\n",
        "for i in range(9):\n",
        "    # Definimos el subplot\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    # Pintamos los datos en formato RAW\n",
        "    plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "# Mostrar el dibujo\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pZOtui7Tozi"
      },
      "source": [
        "Como vamos a querer ir haciendo validación a la vez que entrenamos (muy practico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMnDLvSQTozi"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vQSbC2MTozi"
      },
      "source": [
        "Cuando añadimos convoluciones, necesitamos \"pre-tratar los datos\", porque **la convolución es pera una matriz de 4 campos** (más parecido a \"imagenes\"), no un array de 3 como son actualmente los data sets de train, test y validación. Así que toca darle al **.reshape()**\n",
        "\n",
        "Por eso, al salir de la Convolution, hay que hacer un Flatten, porque las capas FullDense esperan arrays, no matrices!!\n",
        "\n",
        "Luego lo imprimimos para ver que todo está correcto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPSfhtH5Tozi"
      },
      "outputs": [],
      "source": [
        "x_train_cnn = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test_cnn = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "x_val_cnn = x_val.reshape((x_val.shape[0], 28, 28, 1))\n",
        "\n",
        "\n",
        "print('Train: X=%s, y=%s' % (x_train_cnn.shape, y_train.shape))\n",
        "print('Test: X=%s, y=%s' % (x_test_cnn.shape, y_test.shape))\n",
        "print('Validation: X=%s, y=%s' % (x_val_cnn.shape, y_val.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8onNPUAgTozi"
      },
      "source": [
        "## 5. Definamos los callbacks para el Early Stopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiX6KT3eTozi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX0I3KOETozj"
      },
      "source": [
        "## 6. Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-5EDKRDTozj"
      },
      "outputs": [],
      "source": [
        "t = time.perf_counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asd6oJLsTozj"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train_cnn, y_train, epochs=300,\n",
        "                    validation_data=(x_val_cnn, y_val),\n",
        "                    batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpozPbdyTozj"
      },
      "outputs": [],
      "source": [
        "elapsed_time = datetime.timedelta(seconds=(time.perf_counter() - t))\n",
        "\n",
        "print('Tiempo de entrenamiento:', elapsed_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYTP7glTozj"
      },
      "source": [
        "## 6. Evaluamos los resultados\n",
        "\n",
        "Obtengamos una grafica de como el error y la accuracy van evolucionando en cada epoch en los datos de entrenamiento y en la validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbsnhRE8Tozj"
      },
      "outputs": [],
      "source": [
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='train')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "plt.show()\n",
        "\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Wx5PbqTozj"
      },
      "source": [
        "Evaluemos el modelo contra los valores de testeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKkK3FvtTozk"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test_cnn,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE0GOM9ITozk"
      },
      "source": [
        "El coste podemos ver que es estable y es en el epoch 15 que se fija en un valor. Además la accuracy baila alrededor de 0.10. No se puede decir que sea un modelo muy bueno.\n",
        "\n",
        "Veamos que tipo de predicciones estoy obteniendo sobre el conjunto de test (vamos a pintar las imagenes y sus clasificaciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTWHlzPBTozk"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GU_9o9TTozk"
      },
      "source": [
        "Una de las ventajas de Python es que hay montones de funciones y código realizado por terceras personas. Aquí me he fusilado unas bonitas funciones (que he adaptado un poco a mis necesidades) para poder pintar las imagenes, su label (ground truth) y las clasificaciones que hemos realizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyYeMLHATozk"
      },
      "outputs": [],
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n",
        "                                100*np.max(predictions_array),\n",
        "                                true_label),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHZCojlLTozk"
      },
      "source": [
        "Dibujamos los primeros digitos, con las predicciones y sus valores reales (un total de 20 imagenes, para no abusar de vuestros laptops)\n",
        "\n",
        "Coloreamos las prediciones correctas en azul y los fallos en rojo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-_ITmVxTozk"
      },
      "outputs": [],
      "source": [
        "num_rows = 5\n",
        "num_cols = 4\n",
        "start = 650\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i+start, predictions[i+start], y_test, x_test)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i+start, predictions[i+start], y_test)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gcMayntTozl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruEBsT73Tozl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGBcZIOjTozl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}